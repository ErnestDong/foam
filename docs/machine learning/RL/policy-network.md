---
tags: DeepLearning
---

# policy-network

想法是拟合出来一个策略函数$\pi(a|s;\theta)$，给定 state 可以给出 action 的概率分布，再从 action 中随机抽样出来

state-value function 定义为

$$
V(s;\theta)=\sum_{a}\pi(a|s;\theta)Q_{\pi}(s,a)
$$

最大化 $J(\theta)=E_S(V(s;\theta))$

## policy gradient ascend

策略梯度为$\displaystyle\frac{\partial V}{\partial \theta}$

- 对于离散的策略梯度
  - $$\frac{\partial V(s;\theta)}{\partial \theta}=\sum_{a} \frac{\partial \pi(a|s;\theta)}{\partial \theta}Q_\pi(s,a)$$
- 对于连续的策略梯度
  - $$\frac{\partial V(s;\theta)}{\partial \theta}=E_{A\in\pi}(\frac{\partial \log(\pi(A|s,\theta))}{\partial \theta}Q_\pi(s,A))$$
  - 此时需要随机抽取 action 做蒙特卡洛模拟计算期望

算法是

1. 观察 $s_t$
2. 根据策略函数 $\pi$ 抽样 action $a$
3. 计算$q_t=Q_\pi(s_t,a_t)$。不知道 Q，解决方法为
   1. reinforce：记录轨迹用 return u 计算 Q
   2. 用神经网络近似 $Q$，这个神经网络称为 critic，这种方法为 [[#actor-critic method]]
4. 对策略网络求导 $d_{\theta,t}=\frac{\partial \log \pi}{\partial \theta}|_{\theta=\theta_t}$
5. 策略梯度 $g(a_t,\theta_t)=q_t \cdot d_{\theta,t}$
6. 更新网络参数 $\theta$

## actor-critic method

actor 采用[[DQN|value-network]]，critic 采用 [[policy-network]]，同时训练策略网络和价值网络

[//begin]: # "Autogenerated link references for markdown compatibility"
[#actor-critic method]: policy-network.md "policy-network"
[DQN|value-network]: DQN.md "Deep Q-Network"
[policy-network]: policy-network.md "policy-network"
[//end]: # "Autogenerated link references"
