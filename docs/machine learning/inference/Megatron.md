---
tags: DeepLearning
---

# Megatron

## 张量并行

- 一个层中间的张量分成多个部分，分别在不同的 GPU 上计算，然后再合并。
- 每个 GPU 拿到的计算量是相对均匀的，不像[[GPipe]]按照层来划分，有的 GPU 计算量很大，有的很小。

通讯量很大，而且不能做异步通讯，主要是单机多卡的情况，多机情况下还是[[Parameter Server]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[GPipe]: GPipe.md "GPipe"
[Parameter Server]: <Parameter Server.md> "Parameter Server"
[//end]: # "Autogenerated link references"
