---
tags: Hardware
---

# 信息论

信息的本质是对不确定性的消除

## 信息熵

- 一个随机变量的熵越大，意味着不确定性越大，该随机变量包含的信息量越大；
  - 离散熵的定义为：$H(X) = -\sum_{i=1}^{n} p(x_i) \log p(x_i)$
- 熵是平均意义上对随机变量的编码长度

### 互信息

- 互信息是两个随机变量之间的相关性的度量，互信息越大，两个随机变量之间的相关性越大；
  - 互信息的定义为：$I(X;Y) = \sum_{x \in X} \sum_{y \in Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$
    - $I(X;Y) = H(X) - H(X|Y)$，互信息表示为知道事实$Y$后，原来信息量$X$减少了多少。
  - 本质上其实是描述了联合分布$p(x,y)$，与两个边缘分布$p(x)$$p(y)$之积的差异程度，如果差异程度为 0，表示两个随机变量独立，符合我们的直觉。
- 信息链是一个信息传递的过程，信息在链中的传递不会增加，只会减少

## 香农定理

- 香农定理是信息论的基础定理，它描述了在一个信道中，信息的传输速率的上限；
  - 信道容量的定义为：$C = \max_{p(x)} I(X;Y)$
    - 其中$p(x)$是输入信号的概率分布，$I(X;Y)$是输入信号和输出信号之间的互信息
  - 信道容量是一个信道的最大传输速率，即在不出错的情况下，信道可以传输的最大信息量
- $C = B \log_2(1 + \frac{S}{N})$，其中$B$是信道的带宽，$S/N$是信号的信噪比
