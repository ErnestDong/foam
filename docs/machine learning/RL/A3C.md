---
tags: DeepLearning
---

# Asynchronous Advantage Actor-Critic

结合了 [[policy-network]] (Actor) 和 [[DQN|Function Approximation]] (Critic) 的方法. `Actor`  基于概率选行为, `Critic`  基于  `Actor`  的行为评判行为的得分, `Actor`  根据  `Critic`  的评分修改选行为的概率，让 Policy gradient 单步更新

A3C 采用了平行训练的方式。会创建多个并行的环境, 让多个拥有副结构的 agent 同时在这些并行环境上更新主结构中的参数. 并行中的 agent 们互不干扰, 而主结构的参数更新受到副结构提交更新的不连续性干扰, 所以更新的相关性被降低, 收敛性提高。

[//begin]: # "Autogenerated link references for markdown compatibility"
[policy-network]: policy-network.md "policy-network"
[DQN|Function Approximation]: DQN.md "Deep Q-Network"
[//end]: # "Autogenerated link references"
