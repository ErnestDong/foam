---
tags: DeepLearning,Cpp
---

# CUDA 编程

## 语法

CUDA C/C++ 文件扩展名为 `.cu`，使用`nvcc`编译

```cpp
void CPUFunction() {
    printf("This function is defined to run on the CPU.\n");
}

__global__ void GPUFunction() {
    printf("This function is defined to run on the GPU.\n");
}

int main() {
    CPUFunction();
    GPUFunction<<<1, 1>>>();
    cudaDeviceSynchronize();
}
```

其中

- `__global__ void GPUFunction()`
  - `__global__`关键字表明以下函数将在 GPU 上运行并可全局调用，而在此种情况下，则指由 CPU 或 GPU 调用
  - 通常，我们将在 CPU 上执行的代码称为主机代码(host)，而将在 GPU 上运行的代码称为设备代码(device)
  - 使用 `__global__`关键字定义的函数需要返回`void`类型
- `GPUFunction<<<1, 1>>>()`
  - 通常，当调用要在 GPU 上运行的函数时，我们将此种函数称为核函数
  - <<< ... >>>语法提供执行配置，即线程块的数量和每个线程块内的线程数
- `cudaDeviceSynchronize()`
  - 核函数启动方式为异步：CPU 代码将继续执行而无需等待核函数完成启动。
  - 调用 CUDA 运行时提供的函数 `cudaDeviceSynchronize` 可以使主机（CPU）代码暂作等待，直至设备（GPU）代码执行完成，然后再恢复主机代码的执行

可以使用`malloc`分配内存，但通过 malloc 得到的内存只能在 CPU 中访问，需要在 CPU 和 GPU 中同时访问需要通过 `cudaMallocManaged` 函数来分配，即：

```cpp
int *a;
size_t size = n * sizeof(int);
a = (int*)malloc(size) -> cudaMallocManaged(&a, size)
```

## SIMT 单指令多线程

CUDA 的线程层次结构从大到小为 Grid -> Thread Block -> Thread。每个线程（Thread）中都运行一份核函数中内容，多个线程组成一个线程块（Thread Block），与核函数启动关联的所有块组成网格（Grid）

启动核函数时进行的配置就是 `<<<线程块数, 每个块中线程数>>>`，其中每块中线程数一般最大为 1024。在核函数中可以直接访问 CUDA 提供的线程层次结构变量，常用的有：

- `gridDim.x`：Grid 中的 Block 数
- `blockIdx.x`：当前 Block 在 Grid 中的索引
- `blockDim.x`：每个 Block 中的 Thread 数
- `threadIdx.x`：当前 Thread 在所在的 Block 中的索引

总线程数少于要处理的数据数时，每个线程要处理的下一条数据的索引为 `blockIdx.x * blockDim.x + threadIdx.x`，因此 cuda 中的 `for` 循环一般为

```cpp
__global__ void kernel(int *a, int N) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    int stride = gridDim.x * blockDim.x;
    for (int i = idx; i < N; i += stride) {
        // a[i] ...
    }
}
```

## 挑战者

- [[OpenMP]]
- [[MPI]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[OpenMP]: OpenMP.md "OpenMP"
[MPI]: MPI.md "MPI"
[//end]: # "Autogenerated link references"
