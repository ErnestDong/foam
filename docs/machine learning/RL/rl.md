---
tags: DeepLearning
---

# Reinforce Learning

[[神经网络]] Action = f(Observation)，Action 作用于环境产生出 reward ，
讨论的问题是 agent 怎么在复杂、不确定的 environment 中最大化它能获得的 rewards。
agent 的奖励是被延迟的，因而计算 return 时会给 reward 一个折现因子
agent 有以下的组成：

- **策略（policy）**  agent 会用策略来选取下一步的动作 action。
  - 是随机取样的(大多数情况)，也可以是取概率最大的(很少)
  - $\pi(a|s)=P(Action=a|State=s)$，s 状态下使用 action a 的概率，强化学习学的之一就是这个函数 ([[policy-network|policy-based-learning]])
- **价值函数（value function）** 是对未来奖励的预测，用来评估状态的好坏。
  - 使用策略 $\pi$ 时，根据状态 s 和动作 a 的一个价值函数 Q
  - Q 的定义为 t 时刻未来 return 的在采用(s,a)后的期望，强化学习学的函数之二是 $Q^*=\max_{\pi}Q_{\pi}(s,a)$ ([[DQN|value-based-learning]])
- 二者结合则为 [[A3C|actor-critic]] 模型

[//begin]: # "Autogenerated link references for markdown compatibility"
[神经网络]: ../concept/神经网络.md "神经网络"
[policy-network|policy-based-learning]: policy-network.md "policy-network"
[DQN|value-based-learning]: DQN.md "Deep Q-Network"
[//end]: # "Autogenerated link references"
