---
tags: DeepLearning
---

# GPT

GPT-1、GPT-2、GPT-3 的网络架构基本类似，仅在层数方向上增加。

|                  | GPT-1    | GPT-2    | GPT-3      |
| ---------------- | -------- | -------- | ---------- |
| 发布时间         | 2018/6   | 2019/2   | 2020/5     |
| Transformer 层数 | 12       | 48       | 96         |
| 参数量           | 1.17 亿  | 15 亿    | 1750 亿    |
| 训练数据大小     | 5GB      | 40GB     | 45TB       |
| 开源             | 是       | 是       | 否         |
| 输入构造方式     | 特殊标记 | 自然语言 | 自然语言   |
| 微调             | 需要     | 需要     | 可以不需要 |

## GPT-1

基于 [[Transformer]] Decoder 的架构，擅长续写、回答等生成类任务}。GPT 尝试给定一段序列预测下一个词，训练方式和 Decoder 是一样的，都遮盖了下文，只能看到上文，然后不断训练而成。应用到下游任务做微调时，用户只需要**构造 GPT 的输入**，并在 GPT 的输出后**微调**就可以。微调的输出结果是一个概率分布，因此 GPT 每次输出结果可能都不一样。

## GPT-2

允许的输入更像是自然语言，例如“Tweet: I am feeling well. Sentiment:”，而不用\<Start>\<Extract>等特殊的标记。OpenAI 认为在文本中，类似“将下面的句子翻译成中文：Hello world”的句子很常见，当模型足够强大、数据量足够大时 GPT 可以理解自然语言的目的，而不用去构造输入，只需要微调。

> 输入格式完全放开的、能够较好完成下游任务的 AI 模型还要等到 2021-2022 年 Instruction tuning

## GPT-3

更更大，用更大的模型取代了微调的必要

## 基于 GPT 的模型

- codex：加入了 GitHub 的数据训练，可以生成代码
- [[DALLE]]：初代基于 GPT，DALLE 2 基于[[CLIP]]

## GPT-3.5 & ChatGPT

加入了[[RLHF]]后的 InstructGPT 为 GPT3.5，ChatGPT 为 GPT3.5 的一个应用，可以聊天

[//begin]: # "Autogenerated link references for markdown compatibility"
[Transformer]: ../concept/transformer.md "Transformer"
[DALLE]: DALLE.md "DALL E 2"
[CLIP]: CLIP.md "CLIP"
[RLHF]: ../concept/RLHF.md "RLHF"
[//end]: # "Autogenerated link references"
