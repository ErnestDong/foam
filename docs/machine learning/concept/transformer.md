---
tags: DeepLearning
bibliography: [../../reference.bib]
---

# Transformer

## 原理

$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
Q: Query，K: Key，V: Value，d 为归一化以稳定梯度

相关性大的， $\alpha$ 更大，对 `b` 的贡献更强

两个输入的 $alpha$ 计算方式是 $q^i=W^qa^i$ 亦可写作 $Q=W^qI$ 其中 I 是 a 排列的矩阵，Q 是 $alpha$ 排列的矩阵，k/v 类似。 最后 $\alpha=q \cdot k$ 写作矩阵为 $A=K^TQ$ ，A 里面存的分数为 $alpha$ ，对 A 做 `softmax` 后得到 $A^\prime$ 输出为 $B=VA^\prime$

也就是说，只有 $W^*$ 是需要从训练集里面找出来的

如果 attention 限定在较小的范围内（local attention），那么和 CNN 其实差不多。

mask 机制是指产生输出的时候不能看后面的输入，方法是让后面的 key 定为很小的数，从而 softmax 之后接近于 0

## 理解 Attention

- 先让 encoder 找出一个高维向量 $c$ 然后再用 $c$ 逐个 decoder 出来，只不过这个$c$只包含最关注的部分
- 这个$c$需要找到每个里面最关注的东西（c 所包含信息的一部分），关注是不能“交换”的有方向的，因此就有了 query 和 key 这两个概念
- 把一个 token 的向量 $a$ 用矩阵 $Wq$/$Wk$ 映射到两个空间就是 `query` 和 `key`，计算一下他们的相似度。Google 论文里面计算相似度的方式是点乘，也有别的计算方式但是大家都用他的方式了
- 再根据映射到 `value` 空间的那个 `V` 加权，就是 attention 了
- $Wq$、$Wk$、$Wv$ 都是训练时候学习的目标

## Self-Attention

查找内部的关系，自身不同部分的 attention

## 多头注意力机制

注意力机制的叠加，类似于 CNN 的多个核提取不同的信息，输入向量映射到不同的子表达空间中。

## transformer

### encoder

多层 block 来组成[[神经网络]]，输入一个向量输出一个向量，输入向量有多长输出就有多长

每个 block

- 先做 self-attention，然后 add&norm 。所谓 add ，其 output 结果再加上输入的向量（称为 residual connection ），所谓 norm 做 layer normalization （(x-mean)/std） ，
- 然后 fully connect 并再次 add&norm

### decoder

decoder 看到的输入是输入（输入=begin token + output embedding）加上前一个 decoder 的输出

block 和 encoder 的区别在于：

- self-attention 加 mask。所谓 mask 是指产生输出的时候不能看后面的输入，因为 decoder 是一个个输入的，训练为了并行化输入结果因而需要 mask
- 中间夹了一层 cross-attention ，连接 encoder 和 decoder

decoder 可以自己决定输出的长度，方法是输出 token 为 end 时候停止

### encoder-decoder

q 来自 decoder ，k 和 v 来自 encoder ，做 attention 作为 cross-attention

## 应用

- [[BERT]]：预训练的带掩码的大模型，Encoder-only
- [[GPT]]：预训练的大模型，Decoder-only
- [[Pathways]]：“下一代 AI 框架”
- [[ViT]]：transformer 在 CV 中的应用
- [[Swin]]：移动窗口的 attention
- [[CLIP]]：多模态学习

[//begin]: # "Autogenerated link references for markdown compatibility"
[神经网络]: 神经网络.md "神经网络"
[bert]: ../model/BERT.md "BERT"
[gpt]: ../model/GPT.md "GPT"
[pathways]: ../model/Pathways.md "Pathways"
[vit]: ../model/ViT.md "ViT"
[swin]: ../model/Swin.md "Swin"
[clip]: ../model/CLIP.md "CLIP"
[//end]: # "Autogenerated link references"
