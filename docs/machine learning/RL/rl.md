---
tags: DeepLearning
---

# Reinforce Learning

[[神经网络]] Action = f(Observation)，Action 作用于环境产生出 reward ，
讨论的问题是 agent 怎么在复杂、不确定的 environment 中最大化它能获得的 awards。[@wang2022easyrl]
agent 玩游戏的过程改进成端到端训练（end-to-end training）的过程。不需要设计特征，直接输入状态就可以输出动作。可以用一个神经网络来拟合价值函数或策略网络，省去特征工程的过程。
agent 的奖励是被延迟的，强化学习里面一个重要的课题就是近期奖励和远期奖励的权衡 （trade-off），研究怎么让 agent 取得更多的远期奖励。
agent 有以下的组成：

- **策略（policy）**  agent 会用策略来选取下一步的动作。
  - 是随机取样的(大多数情况)，也可以是取概率最大的(很少)
- **价值函数（value function）** 是对未来奖励的预测，用来评估状态的好坏。
  - 使用策略 $\pi$ 时，根据状态 s 和动作 a 的一个价值函数 Q 的定义为
  - $$
  Q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s, a_{t}=a\right]
  =\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} r_{t+k+1} \mid s_{t}=s, a_{t}=a\right]
  $$
- **模型（model）**。模型决定了下一步的状态。
  - 下一步的状态取决于当前的状态以及当前采取的动作，由状态转移概率和奖励函数两个部分组成。

## [[Markov Decision Process]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[神经网络]: ../神经网络.md "神经网络"
[Markov Decision Process]: <Markov Decision Process.md> "Markov Decision Process"
[//end]: # "Autogenerated link references"
