---
tags: DeepLearning
---
# Auto Encoder

## Self-Supervised learning

不需要标注训练集，让模型把样本 X 分为两类，一类用作训练输出 Y ，一类用作和 Y 比较

## [[Auto Encoder]]

自编码器。encoder g 和 decoder f 来做最小化损失$L(x, f(g(x)))$，从而学习得到针对样本输入x的抽象特征表示z。

### MAE

随机盖住图片中的某些块，然后重构这些像素，类似[[transformer]]中的 BERT 模型。盖住之后训练 encoder，效果和[[Auto Encoder]]的[[ViT]]模型差不多，主要用在迁移学习中

由于盖住减少了数据量，训练这个[[transformer]]训练量还是比较低的

### VAE

[//begin]: # "Autogenerated link references for markdown compatibility"
[transformer]: transformer.md "Transformer"
[Auto Encoder#mask]: <Auto Encoder.md> "Auto Encoder"
[Auto Encoder#Next Sentence Prediction]: <Auto Encoder.md> "Auto Encoder"
[Auto Encoder#Downstream Task]: <Auto Encoder.md> "Auto Encoder"
[Auto Encoder]: <Auto Encoder.md> "Auto Encoder"
[ViT]: ViT.md "ViT"
[//end]: # "Autogenerated link references"
