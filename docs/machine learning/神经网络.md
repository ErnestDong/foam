---
tags: DeepLearning
---
# 神经网络

## 梯度下降

我们用最小二乘法来理解“梯度下降”和“反向传播”

```python
import torch
x = torch.rand([500,1]) # X 是一个 tensor ，可以把他想象成 500x1 的向量
y_true = 3*x+8
learning_rate = 0.05 # learning rate 是每次梯度下降的“步长”
w = torch.rand([1,1], requires_grad=True) # w 和 b 我们要 pytorch 自动求导
b = torch.tensor(0, requires_grad=True, dtype=torch.float32)
for i in range(500):
    y_pred = torch.matmul(x,w)+b # 预测是多少
    loss = (y_true-y_pred).pow(2).mean() # 损失
    if w.grad is not None: # 把上一次的梯度清零
        w.grad.data.zero_()
    if b.grad is not None:
        b.grad.data.zero_()
    loss.backward() # 误差反向传播，得到 w 和 b 的梯度
    w.data = w.data - w.grad*learning_rate # 梯度下降找到新的 w 和 b
    b.data = b.data - b.grad*learning_rate
    if i % 50 == 0:
        print(w.item(), b.item(), loss.item())
```

## 激活函数

神经网络本意是想模仿神经元。高中我们学过神经受到刺激后不一定会产生电信号，而是需要达到阈值后才能产生动作电位。因此当神经网络的输入层收到信号传导给隐藏层后，隐藏层是直接向输出层传导（这样的话通过神经网络线性函数的叠加仍然是一个线性函数），而是要经历一个非线性的“激活函数”，如 `relu` , `sigmoid`, `softsign` ，然后再进行传导。即针对 $X$ 输入，神经元输出会是 $f(W^TX+b)$ 。

利用激活函数，一层神经网络可以拟合出任意的函数，而多层的神经网络在拟合时更有效率

## 损失函数

## 优化器

传统 `SGD` ，无脑 `Adam` 。


