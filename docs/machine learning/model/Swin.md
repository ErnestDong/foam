---
tags: DeepLearning
bibliography: [../../reference.bib]
---
# Swin

将 [[ViT]] 像[[CNN]]一样，分几个 block 做层级式的特征提取，[[transformer]]是一个通用的骨干网络。
让 ViT 可以有全局的建模，计算复杂度没有那么大(随图像大小线性增长)。利用了局部性的性质，在小的窗口上算自注意力。

## Shifted Window

用窗口来做 attention，序列长度永远是窗口大小的平方倍。窗口的通信采用 Shift 的方式，即移动一小部分后 block 再做一个 attention，最后两个 attention+MLP 层连起来

## patch merging

把临接的小 patch 组合为一个大 patch，类似 CNN 里面的 pooling

把 $h \times w \times c$的 tensor分解为4个 $\frac{h}{2} \times \frac{w}{2} \times c$ 的 tensor，然后组合成 $\frac{h}{2} \times \frac{w}{2} \times 4c$，利用一个 1x1 的卷积核降维到 $\frac{h}{2} \times \frac{w}{2} \times 2c$，最后一层 global average pooling 变成 $1\times 1\times x$的向量

## hierarchy

[//begin]: # "Autogenerated link references for markdown compatibility"
[ViT]: ViT.md "ViT"
[CNN]: ../concept/CNN.md "CNN"
[transformer]: ../concept/transformer.md "Transformer"
[//end]: # "Autogenerated link references"
