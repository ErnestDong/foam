---
tags: DeepLearning
---

# LoRA

微调[[transformer]]中的权重矩阵$W_0$时，将其分解为$W=W_0+AB$，其中$A$、$B$是一个低秩矩阵。微调一个 390G 的模型，只需要一个 3.9M 的 LoRA

[//begin]: # "Autogenerated link references for markdown compatibility"
[transformer]: transformer.md "Transformer"
[//end]: # "Autogenerated link references"
