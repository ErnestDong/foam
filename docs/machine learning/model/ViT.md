---
tags: DeepLearning
bibliography: [../../reference.bib]
---
# ViT

把 attention 放到 CV 领域，想办法降低序列长度

把图片打成很多 16x16 的 patch，把 patch 看成单词，直接应用 [[transformer]]

## ViLT

多模态的ViT模型，图像和文本分别经过一个 linear embedding 层之后用一个 transformer 解决

[//begin]: # "Autogenerated link references for markdown compatibility"
[transformer]: ../concept/transformer.md "Transformer"
[//end]: # "Autogenerated link references"
