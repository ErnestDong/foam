---
tags: DeepLearning
bibliography: [../../reference.bib]
---

# Attention

## 原理

$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
Q: Query，K: Key，V: Value，d 为归一化以稳定梯度

相关性大的， $\alpha$ 更大，对 `b` 的贡献更强

两个输入的 $alpha$ 计算方式是 $q^i=W^qa^i$ 亦可写作 $Q=W^qI$ 其中 I 是 a 排列的矩阵，Q 是 $alpha$ 排列的矩阵，k/v 类似。 最后 $\alpha=q \cdot k$ 写作矩阵为 $A=K^TQ$ ，A 里面存的分数为 $alpha$ ，对 A 做 `softmax` 后得到 $A^\prime$ 输出为 $B=VA^\prime$

也就是说，只有 $W^*$ 是需要从训练集里面找出来的

如果 attention 限定在较小的范围内（local attention），那么和 CNN 其实差不多。

mask 机制是指产生输出的时候不能看后面的输入，方法是让后面的 key 定为很小的数，从而 softmax 之后接近于 0

### 理解 Attention

- 先让 encoder 找出一个高维向量 $c$ 然后再用 $c$ 逐个 decoder 出来，只不过这个$c$只包含最关注的部分
- 这个$c$需要找到每个里面最关注的东西（c 所包含信息的一部分），关注是不能“交换”的有方向的，因此就有了 query 和 key 这两个概念
- 把一个 token 的向量 $a$ 用矩阵 $Wq$/$Wk$ 映射到两个空间就是 `query` 和 `key`，计算一下他们的相似度。Google 论文里面计算相似度的方式是点乘，也有别的计算方式但是大家都用他的方式了
- 再根据映射到 `value` 空间的那个 `V` 加权，就是 attention 了
- $Wq$、$Wk$、$Wv$ 都是训练时候学习的目标

### Self-Attention

查找内部的关系，自身不同部分的 attention

## 多头注意力机制

注意力机制的叠加，类似于 CNN 的多个核提取不同的信息，输入向量映射到不同的子表达空间中。

> 改进：Grouped-Query Attention
> 用在 llama 中
> 本质上是将 Q 分组，同组 Q 共享 KV 然后再计算 attention，从而减少 KV-Cache 和显存浪费

## 改进

- Page Attention：利用分页的方式，将逻辑显存打包分页减少 K-V Cache，然后再储存到物理显存中从而减少显存浪费
- Transformer-XL：空间换时间，将上下文信息储存到缓存中，然后再利用缓存信息进行训练，从而实现超长 context window
- Flash Attention：Transformer 的计算瓶颈不在计算上而在 HBM 读写上，通过分块计算控制 I/O，进而解决 memory-bound 的问题，提升整体运算速度。
- Ring Attention：把 Attention 计算拆分到多卡上，有点类似[[Megatron]]的张量并行

[//begin]: # "Autogenerated link references for markdown compatibility"
[Megatron]: ../../inference/Megatron.md "Megatron"
[//end]: # "Autogenerated link references"
