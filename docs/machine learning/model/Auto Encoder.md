---
tags: DeepLearning
---

# Auto Encoder

自编码器。encoder g 和 decoder f 来做最小化损失$L(x, f(g(x)))$，从而学习得到针对样本输入 x 的抽象特征表示 z。

## Self-Supervised learning

不需要标注训练集，让模型把样本 X 分为两类，一类用作训练输出 Y ，一类用作和 Y 比较

### MAE

主要用在迁移学习中。随机盖住图片中的某些块，然后重构这些像素，类似[[BERT]] 模型。mask 盖住之后训练 encoder(原文中 encoder 是一个 [[ViT]])，效果和全部用[[ViT]]模型差不多，但是不需要那么多的数据。

由于盖住减少了数据量，训练这个[[transformer]]训练量还是比较低的

### VAE

输入映射到一个分布上，比如 GMM，分解成均值向量和方差向量，然后运用 Auto Encoder

[//begin]: # "Autogenerated link references for markdown compatibility"
[BERT]: BERT.md "BERT"
[ViT]: ViT.md "ViT"
[transformer]: ../concept/transformer.md "Transformer"
[//end]: # "Autogenerated link references"
